{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Code.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "7MQJ2tS2nI0w",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# http://pytorch.org/\n",
        "from os import path\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "\n",
        "accelerator = 'cu80' if path.exists('/opt/bin/nvidia-smi') else 'cpu'\n",
        "\n",
        "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.3.0.post4-{platform}-linux_x86_64.whl torchvision\n",
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EkT15hrlnS1m",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import torch \n",
        "import torch.nn as nn\n",
        "import torchvision.datasets as dsets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.autograd import Variable"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QE6LKwY5kRq1",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "54b026d8-b5be-4401-93ca-19c85dc72b36",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1524264607850,
          "user_tz": 240,
          "elapsed": 6863,
          "user": {
            "displayName": "Felipee Rincon",
            "photoUrl": "//lh6.googleusercontent.com/-dk2d1rW6GPg/AAAAAAAAAAI/AAAAAAAAAD0/QXdoeNkXzNA/s50-c-k-no/photo.jpg",
            "userId": "114715455584002133434"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# memory footprint support libraries/code\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isn’t guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        "  process = psutil.Process(os.getpid())\n",
        "  print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" I Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        "  print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "printm()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gputil in /usr/local/lib/python2.7/dist-packages\r\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python2.7/dist-packages (from gputil)\n",
            "\u001b[33mYou are using pip version 9.0.3, however version 10.0.1 is available.\n",
            "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python2.7/dist-packages\n",
            "\u001b[33mYou are using pip version 9.0.3, however version 10.0.1 is available.\n",
            "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python2.7/dist-packages\n",
            "\u001b[33mYou are using pip version 9.0.3, however version 10.0.1 is available.\n",
            "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
            "('Gen RAM Free: 9.4 GB', ' I Proc size: 2.4 GB')\n",
            "GPU RAM Free: 923MB | Used: 10516MB | Util  92% | Total 11439MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "KiQyQyzQbDGs",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "#Training VGG-Att3-concat-pc\n",
        "\n",
        "from os import path\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "\n",
        "accelerator = 'cu80' if path.exists('/opt/bin/nvidia-smi') else 'cpu'\n",
        "\n",
        "import torch\n",
        "\n",
        "#######\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "transform10 = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.53129727, 0.5259391, 0.52069134), (0.28938246, 0.28505746, 0.27971658))])\n",
        "batch_size = 128\n",
        "lr = 0.1\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform10)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform10)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "######\n",
        "\n",
        "# memory footprint support libraries/code\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "#GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isn’t guaranteed\n",
        "#gpu = GPUs[0]\n",
        "#def printm():\n",
        "  #process = psutil.Process(os.getpid())\n",
        "  #print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" I Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        "  #print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "#printm()\n",
        "\n",
        "######\n",
        "print(\"Training model VGG-Att3-concat-pc on CIFAR 10\")\n",
        "print(torch.cuda.current_device())\n",
        "print(torch.cuda.device(0))\n",
        "print(torch.cuda.device_count())\n",
        "print(torch.cuda.get_device_name(0))\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "          super(Net, self).__init__()\n",
        "           \n",
        "        \n",
        "       \n",
        "       \n",
        "        \n",
        "          #FIRST LAYER\n",
        "          self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
        "          self.bn1 =  nn.BatchNorm2d(64)\n",
        "          self.drop1 = nn.Dropout2d(p=0.3)\n",
        "          #SECOND LAYER\n",
        "          self.conv2 =nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
        "          self.bn2 =  nn.BatchNorm2d(64)\n",
        "          #THIRD LAYER\n",
        "          self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "          self.bn3 =  nn.BatchNorm2d(128)\n",
        "          self.drop3 = nn.Dropout2d(p=0.4)\n",
        "          #4TH LAYER\n",
        "          self.conv4 =nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
        "          self.bn4 =  nn.BatchNorm2d(128)\n",
        "          \n",
        "          #5TH LAYER\n",
        "          self.conv5 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
        "          self.bn5 =  nn.BatchNorm2d(256)\n",
        "          self.drop5 = nn.Dropout2d(p=0.4)\n",
        "          \n",
        "          #6TH LAYER\n",
        "          self.conv6 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
        "          self.bn6 =  nn.BatchNorm2d(256)\n",
        "          self.drop6 = nn.Dropout2d(p=0.4)\n",
        "          \n",
        "          #7TH LAYER\n",
        "          self.conv7 =nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
        "          self.bn7 =  nn.BatchNorm2d(256)\n",
        "          self.pool7 = nn.MaxPool2d(2, stride=2)\n",
        "          \n",
        "         \n",
        "          #8TH LAYER\n",
        "          self.conv8 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
        "          self.bn8 =  nn.BatchNorm2d(512)\n",
        "          self.drop8 = nn.Dropout2d(p=0.4)\n",
        "        \n",
        "          #9TH LAYER\n",
        "          self.conv9 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "          self.bn9 =  nn.BatchNorm2d(512)\n",
        "          self.drop9 = nn.Dropout2d(p=0.4)\n",
        "           \n",
        "        \n",
        "          #10TH LAYER\n",
        "          self.conv10 =nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "          self.bn10 =  nn.BatchNorm2d(512)\n",
        "          self.pool10 = nn.MaxPool2d(2, stride=2)\n",
        "          \n",
        "       \n",
        "          #11TH LAYER\n",
        "          self.conv11 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "          self.bn11 =  nn.BatchNorm2d(512)\n",
        "          self.drop11 = nn.Dropout2d(p=0.4)\n",
        "           \n",
        "          \n",
        "          #12TH LAYER\n",
        "          self.conv12 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "          self.bn12 =  nn.BatchNorm2d(512)\n",
        "          self.drop12 = nn.Dropout2d(p=0.4)\n",
        "           \n",
        "          \n",
        "          #13TH LAYER\n",
        "          self.conv13 =nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "          self.bn13 =  nn.BatchNorm2d(512)\n",
        "          self.pool13 = nn.MaxPool2d(2, stride=2)\n",
        "         \n",
        "          #14TH LAYER\n",
        "          self.conv14 =nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "          self.bn14 =  nn.BatchNorm2d(512)\n",
        "          self.pool14 = nn.MaxPool2d(2, stride=2)\n",
        "            \n",
        "        \n",
        "          #15TH LAYER\n",
        "          self.conv15 =nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "          self.bn15 =  nn.BatchNorm2d(512)\n",
        "          self.pool15 = nn.MaxPool2d(2, stride=2)\n",
        "           \n",
        "          #16TH LAYER\n",
        "          \n",
        "          self.drop16 = nn.Dropout2d(p=0.5)\n",
        "          self.linear16 = nn.Linear(512, 512)\n",
        "          self.bn16 =  nn.BatchNorm2d(512)\n",
        "         \n",
        "        \n",
        "        \n",
        "        \n",
        "          #self.weightUA = MyModuleUA((1,512)).cuda()#vector u for layer 10\n",
        "          #self.weightUB = MyModuleUB((1,512)).cuda()# vector u for layer 13\n",
        "          self.layerp = nn.MaxPool1d(kernel_size =  2, dilation = 1, padding = 0, stride=2)\n",
        "          self.bnF =  nn.BatchNorm2d(512)\n",
        "          self.fc1 = nn.Linear(512, 512)\n",
        "          self.linearUA = nn.Linear(512, 1, bias = False)\n",
        "          self.linearUB = nn.Linear(512, 1, bias = False)\n",
        "          self.linearUC = nn.Linear(256, 1, bias = False)\n",
        "          #self.dropF = nn.Dropout2d(p=0.5)\n",
        "          self.fc3 = nn.Linear(1024 + 256, 10)\n",
        "\n",
        "            \n",
        "  def forward(self, x):\n",
        "          size = x.size(0)\n",
        "          x = self.drop1(F.relu(self.bn1(self.conv1(x)),inplace = True))#1ST LAYER\n",
        "          x = (F.relu(self.bn2(self.conv2(x)),inplace = True))#2ND LAYER\n",
        "          x = self.drop3(F.relu(self.bn3(self.conv3(x)),inplace = True))#3rd LAYER\n",
        "          x = (F.relu(self.bn4(self.conv4(x)),inplace = True))#4th LAYER\n",
        "          x = self.drop5(F.relu(self.bn5(self.conv5(x)),inplace = True))#5th LAYER\n",
        "          x = self.drop6(F.relu(self.bn6(self.conv6(x)),inplace = True))#6th LAYER\n",
        "          x = self.pool7(F.relu(self.bn7(self.conv7(x)),inplace = True))#7th LAYER\n",
        "          out7 = x.view(x.size(0), x.size(1),-1).clone() #feature vectors from layer7\n",
        "          x = self.drop8(F.relu(self.bn8(self.conv8(x)),inplace = True))#8th LAYER\n",
        "          x = self.drop9(F.relu(self.bn9(self.conv9(x)),inplace = True))#9th LAYER\n",
        "          x = self.pool10(F.relu(self.bn10(self.conv10(x)),inplace = True))#10th LAYER\n",
        "         \n",
        "          out10 = x.view(x.size(0), x.size(1),-1).clone() #feature vectors from layer 10\n",
        "          x = self.drop11(F.relu(self.bn11(self.conv11(x)),inplace = True))#11th LAYER\n",
        "          x = self.drop12(F.relu(self.bn12(self.conv12(x)),inplace = True))#12th LAYER\n",
        "          x = self.pool13(F.relu(self.bn13(self.conv13(x)),inplace = True))#13th LAYER\n",
        "          out13 = x.view(x.size(0), x.size(1),-1).clone() # feature vectors from layer 13\n",
        "          x = self.pool14(F.relu(self.bn14(self.conv14(x)),inplace = True))#14th LAYER\n",
        "          x = self.pool15(F.relu(self.bn15(self.conv15(x)),inplace = True))#15th LAYER\n",
        "          \n",
        "          x = x.view(x.size(0), -1) \n",
        "         \n",
        "          x = (F.relu(self.bn16(self.linear16((self.drop16(x) ))), inplace = True))#16th LAYER\n",
        "         \n",
        "          x = x.view(-1, 512)\n",
        "         \n",
        "          outModified = x.view(x.size(0), x.size(1),1).clone()\n",
        "          outModified2 = x.view(x.size(0),1, x.size(1)).clone()\n",
        "          outModified2 = self.layerp(outModified2)\n",
        "          outModified2 = outModified2.view(outModified2.size(0), outModified2.size(2),1)\n",
        "         \n",
        "          out10add = out10.add(outModified) #L(i)+ g layer 10\n",
        "          out13add = out13.add(outModified)#L(i)+ g layer 13\n",
        "          out7add = out7.add(outModified2)#L(i)+ g layer 7\n",
        "          \n",
        "          \n",
        "          #layer 13 dot product\n",
        "          out13add = out13add.view(out13add.size(0), out13add.size(2), out13add.size(1))\n",
        "          outRes = self.linearUA(out13add)\n",
        "          outRes = outRes.view(outRes.size(0),-1)\n",
        "          outRes = F.softmax(outRes, dim = 1)\n",
        "        \n",
        "        \n",
        "          #layer 13 soft max and weighted addition\n",
        "          outRes2 = outRes.view(outRes.size(0), outRes.size(1), 1) \n",
        "         \n",
        "          outDot2 = torch.bmm(out13, outRes2)#weights*l(i)\n",
        "         \n",
        "          outDot2 = outDot2.view(outDot2.size(0), outDot2.size(1))\n",
        "          \n",
        "          #layer 10 dot product\n",
        "        \n",
        "          out10add = out10add.view(out10add.size(0), out10add.size(2), out10add.size(1))\n",
        "         \n",
        "          outRes3 = self.linearUB(out10add)\n",
        "          outRes3 = outRes3.view(outRes3.size(0),-1)\n",
        "         \n",
        "          outRes3 = F.softmax(outRes3, dim = 1)\n",
        "        \n",
        "          \n",
        "          \n",
        "        \n",
        "          #layer 10 soft max and weighted addition\n",
        "          \n",
        "          outRes4 = outRes3.view(outRes3.size(0), outRes3.size(1), 1)\n",
        "         \n",
        "          outDot4 = torch.bmm(out10, outRes4)#weights*l(i)\n",
        "         \n",
        "          outDot4 = outDot4.view(outDot4.size(0), outDot4.size(1))\n",
        "          \n",
        "           #layer 7 dot product\n",
        "        \n",
        "          out7add = out7add.view(out7add.size(0), out7add.size(2), out7add.size(1))\n",
        "         \n",
        "          outRes5 = self.linearUC(out7add)\n",
        "          outRes5 = outRes5.view(outRes5.size(0),-1)\n",
        "         \n",
        "          outRes5 = F.softmax(outRes5, dim = 1)\n",
        "        \n",
        "         \n",
        "          \n",
        "        \n",
        "          #layer 7 soft max and weighted addition\n",
        "          \n",
        "          outRes6 = outRes5.view(outRes5.size(0), outRes5.size(1), 1)\n",
        "         \n",
        "          outDot6 = torch.bmm(out7, outRes6)#weights*l(i)\n",
        "         \n",
        "          outDot6 = outDot6.view(outDot6.size(0), outDot6.size(1))\n",
        "        \n",
        "          #concat layers\n",
        "         \n",
        "          outConcat = torch.cat((outDot2, outDot4, outDot6), dim = 1)\n",
        "          \n",
        "          outConcat = outConcat.view(outConcat.size(0),-1)\n",
        "          \n",
        "        \n",
        "          x = F.softmax((self.fc3((outConcat)) ), dim =1)\n",
        "          \n",
        "          return x\n",
        "\n",
        "\n",
        "\n",
        "import torch.backends.cudnn as cudnn\n",
        "net = Net().cuda()\n",
        "net = torch.nn.DataParallel(net, device_ids=range(torch.cuda.device_count()))\n",
        "cudnn.benchmark = True\n",
        "import torch.optim as optim\n",
        "import time\n",
        "\n",
        "\n",
        "\n",
        "criterion = nn.CrossEntropyLoss().cuda()\n",
        "optimizer = optim.SGD(net.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
        "start = time.time()\n",
        "logfile = open('vgg-att3-concat-pc.csv', 'a+')\n",
        "dataheader = 'epoch, iteration, loss, train-acc, lr, time \\n'\n",
        "logfile.write(dataheader)\n",
        "logfile.close()\n",
        "for epoch in range(300):  # loop over the dataset multiple times\n",
        "    \n",
        "    if epoch %25 == 0 :\n",
        "        if epoch != 0:\n",
        "            lr = lr/2.0\n",
        "\n",
        "    optimizer = optim.SGD(net.parameters(), lr=lr, momentum=0.9, weight_decay = 5e-4)\n",
        "\n",
        "    running_loss = 0.0\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    torch.save(net.state_dict(), 'checkpointModelAtt3.pkl')\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "            # get the inputs\n",
        "            inputs, labels = data\n",
        "            #print(len(labels))\n",
        "            # wrap them in Variable\n",
        "            inputs, labels = Variable(inputs).cuda(), Variable(labels).cuda(async = True)\n",
        "            \n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            # forward + backward + optimize\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            # print statistics\n",
        "            running_loss += loss.data[0]\n",
        "            total += labels.size(0)\n",
        "            \n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            correct += predicted.eq(labels.data).cpu().sum()\n",
        "            \n",
        "            if i % 10 == 9:    # print every 10 mini-batches\n",
        "                        end = time.time()\n",
        "                        timetaken = (end - start)\n",
        "                \n",
        "                        data = ('[%d, %5d] loss: %.3f, Acc: %.3f%% (%d/%d), Learning-rate:%.8f' %\n",
        "                        (epoch + 1, i + 1, running_loss / 10,100.*correct/total, correct, total, lr))\n",
        "                        \n",
        "                        print(str(data) + ' time: ' + str(timetaken))\n",
        "                        \n",
        "                        dataline = ('%d , %5d ,  %.3f ,  %.3f , %.8f , %.8f' %\n",
        "                        (epoch + 1 +250, i + 1, running_loss / 10,100.*correct/total, lr, timetaken))\n",
        "                        \n",
        "                        logfile = open('vgg-att3-concat-pc.csv', 'a+')\n",
        "                        logfile.write(str(dataline) + '\\n')\n",
        "                        logfile.close()\n",
        "                        start = time.time()\n",
        "                        running_loss = 0.0\n",
        "                        total = 0\n",
        "                        correct = 0\n",
        "                        \n",
        "            \n",
        "    lr = lr - 1e-7\n",
        "    if lr <= 0:\n",
        "\t     break;"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rTZen2SezyXy",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "#testing vgg-att3-concat-pc model\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform10)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=100,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "import torch.backends.cudnn as cudnn\n",
        "net = Net().cuda()\n",
        "net = torch.nn.DataParallel(net, device_ids=range(torch.cuda.device_count()))\n",
        "cudnn.benchmark = True\n",
        "import torch.optim as optim\n",
        "import time\n",
        "\n",
        "dict_loaded = torch.load('checkpointModelAtt3.pkl')\n",
        "dict_keys_loaded = dict_loaded.keys()\n",
        "new_keys = net.state_dict().keys()\n",
        "dictionary = dict(zip(dict_keys_loaded, new_keys))\n",
        "\n",
        "new_dict = dict((dictionary[key], value) for (key, value) in dict_loaded.items())\n",
        "print(new_dict.keys())\n",
        "net.load_state_dict((new_dict ))\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "def plot_confusion_matrix(cls_pred, cls_true):\n",
        "        num_classes = 10\n",
        "        cm = confusion_matrix(y_true=cls_true,\n",
        "                              y_pred=cls_pred)\n",
        "        plt.imshow(cm, interpolation='nearest',cmap=plt.cm.jet)\n",
        "        plt.colorbar()\n",
        "        tick_marks = np.arange(num_classes)\n",
        "        plt.xticks(tick_marks, range(num_classes))\n",
        "        plt.yticks(tick_marks, range(num_classes))\n",
        "        #plt.titlr('Confusion Matrix')\n",
        "        plt.xlabel('Predicted class')\n",
        "        plt.ylabel('True class')\n",
        "        plt.show()\n",
        "        \n",
        "        \n",
        "\n",
        "test_loss = 0\n",
        "correct = 0\n",
        "total = 0\n",
        "net.eval()\n",
        "preds = []\n",
        "trues = []\n",
        "for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "        \n",
        "        inputs, targets = Variable(inputs).cuda(), Variable(targets).cuda(async = True)\n",
        "\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        test_loss += loss.data[0]\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        preds +=(Variable(predicted).data).cpu().numpy().tolist()\n",
        "        trues += targets.data.cpu().numpy().tolist()\n",
        "        \n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets.data).cpu().sum()\n",
        "        \n",
        "        data = ( 'Loss: %.3f | Acc: %.3f%% (%d/%d)' % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
        "        print(data)\n",
        "          \n",
        "print(preds)\n",
        "print(trues)\n",
        "\n",
        "print(data)\n",
        "plt.figure()\n",
        "plot_confusion_matrix(preds, trues)\n",
        "unique, counts = np.unique(trues, return_counts=True)\n",
        "dict(zip(unique, counts))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xRPmDXFSLdKa",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "from os import path\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "\n",
        "accelerator = 'cu80' if path.exists('/opt/bin/nvidia-smi') else 'cpu'\n",
        "\n",
        "import torch\n",
        "\n",
        "#######\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "transform10 = transforms.Compose(\n",
        "                                 [transforms.ToTensor(),\n",
        "                                  transforms.Normalize((0.53129727, 0.5259391, 0.52069134), (0.28938246, 0.28505746, 0.27971658))])\n",
        "batch_size = 128\n",
        "lr = 0.1\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform10)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform10)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "######\n",
        "\n",
        "# memory footprint support libraries/code\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "#GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isn’t guaranteed\n",
        "#gpu = GPUs[0]\n",
        "#def printm():\n",
        "#process = psutil.Process(os.getpid())\n",
        "#print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" I Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        "#print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "#printm()\n",
        "\n",
        "######\n",
        "print(\"Training model VGG-Att2-concat-pc on CIFAR 10\")\n",
        "print(torch.cuda.current_device())\n",
        "print(torch.cuda.device(0))\n",
        "print(torch.cuda.device_count())\n",
        "print(torch.cuda.get_device_name(0))\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        #FIRST LAYER\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
        "        self.bn1 =  nn.BatchNorm2d(64)\n",
        "        self.drop1 = nn.Dropout2d(p=0.3)\n",
        "        #SECOND LAYER\n",
        "        self.conv2 =nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
        "        self.bn2 =  nn.BatchNorm2d(64)\n",
        "        #THIRD LAYER\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.bn3 =  nn.BatchNorm2d(128)\n",
        "        self.drop3 = nn.Dropout2d(p=0.4)\n",
        "        #4TH LAYER\n",
        "        self.conv4 =nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
        "        self.bn4 =  nn.BatchNorm2d(128)\n",
        "        \n",
        "        #5TH LAYER\n",
        "        self.conv5 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
        "        self.bn5 =  nn.BatchNorm2d(256)\n",
        "        self.drop5 = nn.Dropout2d(p=0.4)\n",
        "        \n",
        "        #6TH LAYER\n",
        "        self.conv6 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
        "        self.bn6 =  nn.BatchNorm2d(256)\n",
        "        self.drop6 = nn.Dropout2d(p=0.4)\n",
        "        \n",
        "        #7TH LAYER\n",
        "        self.conv7 =nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
        "        self.bn7 =  nn.BatchNorm2d(256)\n",
        "        self.pool7 = nn.MaxPool2d(2, stride=2)\n",
        "        \n",
        "        \n",
        "        #8TH LAYER\n",
        "        self.conv8 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
        "        self.bn8 =  nn.BatchNorm2d(512)\n",
        "        self.drop8 = nn.Dropout2d(p=0.4)\n",
        "        \n",
        "        #9TH LAYER\n",
        "        self.conv9 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "        self.bn9 =  nn.BatchNorm2d(512)\n",
        "        self.drop9 = nn.Dropout2d(p=0.4)\n",
        "        \n",
        "        \n",
        "        #10TH LAYER\n",
        "        self.conv10 =nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "        self.bn10 =  nn.BatchNorm2d(512)\n",
        "        self.pool10 = nn.MaxPool2d(2, stride=2)\n",
        "        \n",
        "        \n",
        "        #11TH LAYER\n",
        "        self.conv11 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "        self.bn11 =  nn.BatchNorm2d(512)\n",
        "        self.drop11 = nn.Dropout2d(p=0.4)\n",
        "        \n",
        "        \n",
        "        #12TH LAYER\n",
        "        self.conv12 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "        self.bn12 =  nn.BatchNorm2d(512)\n",
        "        self.drop12 = nn.Dropout2d(p=0.4)\n",
        "        \n",
        "        \n",
        "        #13TH LAYER\n",
        "        self.conv13 =nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "        self.bn13 =  nn.BatchNorm2d(512)\n",
        "        self.pool13 = nn.MaxPool2d(2, stride=2)\n",
        "        \n",
        "        #14TH LAYER\n",
        "        self.conv14 =nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "        self.bn14 =  nn.BatchNorm2d(512)\n",
        "        self.pool14 = nn.MaxPool2d(2, stride=2)\n",
        "        \n",
        "        \n",
        "        #15TH LAYER\n",
        "        self.conv15 =nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "        self.bn15 =  nn.BatchNorm2d(512)\n",
        "        self.pool15 = nn.MaxPool2d(2, stride=2)\n",
        "        \n",
        "        #16TH LAYER\n",
        "        \n",
        "        self.drop16 = nn.Dropout2d(p=0.5)\n",
        "        self.linear16 = nn.Linear(512, 512)\n",
        "        self.bn16 =  nn.BatchNorm2d(512)\n",
        "            \n",
        "            \n",
        "            \n",
        "     \n",
        "        \n",
        "        self.bnF =  nn.BatchNorm2d(512)\n",
        "        self.fc1 = nn.Linear(512, 512)\n",
        "        self.linearUA = nn.Linear(512, 1, bias = False)\n",
        "        self.linearUB = nn.Linear(512, 1, bias = False)\n",
        "        \n",
        "        self.fc3 = nn.Linear(1024, 10)\n",
        "                    \n",
        "                    \n",
        "    def forward(self, x):\n",
        "        size = x.size(0)\n",
        "        x = self.drop1(F.relu(self.bn1(self.conv1(x)), inplace = True))#1ST LAYER\n",
        "        x = (F.relu(self.bn2(self.conv2(x)), inplace = True))#2ND LAYER\n",
        "        x = self.drop3(F.relu(self.bn3(self.conv3(x)), inplace = True))#3rd LAYER\n",
        "        x = (F.relu(self.bn4(self.conv4(x)), inplace = True))#4th LAYER\n",
        "        x = self.drop5(F.relu(self.bn5(self.conv5(x)), inplace = True))#5th LAYER\n",
        "        x = self.drop6(F.relu(self.bn6(self.conv6(x)), inplace = True))#6th LAYER\n",
        "        x = self.pool7(F.relu(self.bn7(self.conv7(x)), inplace = True))#7th LAYER\n",
        "        x = self.drop8(F.relu(self.bn8(self.conv8(x)), inplace = True))#8th LAYER\n",
        "        x = self.drop9(F.relu(self.bn9(self.conv9(x)), inplace = True))#9th LAYER\n",
        "        x = self.pool10(F.relu(self.bn10(self.conv10(x)), inplace = True))#10th LAYER\n",
        "\n",
        "        out10 = x.view(x.size(0), x.size(1),-1).clone() #feature vectors from layer 10\n",
        "        x = self.drop11(F.relu(self.bn11(self.conv11(x)), inplace = True))#11th LAYER\n",
        "        x = self.drop12(F.relu(self.bn12(self.conv12(x)), inplace = True))#12th LAYER\n",
        "        x = self.pool13(F.relu(self.bn13(self.conv13(x)), inplace = True))#13th LAYER\n",
        "        out13 = x.view(x.size(0), x.size(1),-1).clone() # feature vectors from layer 13\n",
        "        x = self.pool14(F.relu(self.bn14(self.conv14(x)), inplace = True))#14th LAYER\n",
        "        x = self.pool15(F.relu(self.bn15(self.conv15(x)), inplace = True))#15th LAYER\n",
        "\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = (F.relu(self.bn16(self.linear16((self.drop16(x) ))), inplace = True))#16th LAYER\n",
        "    \n",
        "        x = x.view(-1, 512)\n",
        "        x = (F.relu(self.bnF(self.fc1(x)), inplace = True))\n",
        "        outModified = x.view(x.size(0), x.size(1),1).clone()\n",
        "        \n",
        "       \n",
        "        out10add = out10.add(outModified) #L(i)+ g layer 10\n",
        "        out13add = out13.add(outModified)#L(i)+ g layer 13\n",
        "\n",
        "          #layer 13 dot product\n",
        "        out13add = out13add.view(out13add.size(0), out13add.size(2), out13add.size(1))\n",
        "        outRes = self.linearUA(out13add)\n",
        "        outRes = outRes.view(outRes.size(0),-1)\n",
        "            \n",
        "        outRes = F.softmax(outRes, dim = 1)\n",
        "               \n",
        "                \n",
        "                 #layer 13 soft max and weighted addition\n",
        "        outRes2 = outRes.view(outRes.size(0), outRes.size(1), 1)\n",
        "                \n",
        "        outDot2 = torch.bmm(out13, outRes2)#weights*l(i)\n",
        "                    \n",
        "        outDot2 = outDot2.view(outDot2.size(0), outDot2.size(1))\n",
        "                    \n",
        "                    #layer 10 dot product\n",
        "                    \n",
        "        out10add = out10add.view(out10add.size(0), out10add.size(2), out10add.size(1))\n",
        "       \n",
        "        outRes3 = self.linearUB(out10add)\n",
        "        outRes3 = outRes3.view(outRes3.size(0),-1)\n",
        "        \n",
        "        outRes3 = F.softmax(outRes3, dim = 1)\n",
        "          \n",
        "            \n",
        "            \n",
        "            #layer 10 soft max and weighted addition\n",
        "            \n",
        "        outRes4 = outRes3.view(outRes3.size(0), outRes3.size(1), 1)\n",
        "            \n",
        "        outDot4 = torch.bmm(out10, outRes4)#weights*l(i)\n",
        "                \n",
        "        outDot4 = outDot4.view(outDot4.size(0), outDot4.size(1))\n",
        "                \n",
        "                \n",
        "                #concat layers\n",
        "                \n",
        "        outConcat = torch.cat((outDot2, outDot4), dim = 1)\n",
        "                    \n",
        "        outConcat = outConcat.view(outConcat.size(0),-1)\n",
        "                    \n",
        "                    \n",
        "        x = F.softmax((self.fc3((outConcat)) ), dim =1)\n",
        "                        \n",
        "        return x\n",
        "\n",
        "import torch.backends.cudnn as cudnn\n",
        "net = Net().cuda()\n",
        "net = torch.nn.DataParallel(net, device_ids=range(torch.cuda.device_count()))\n",
        "cudnn.benchmark = True\n",
        "import torch.optim as optim\n",
        "import time\n",
        "\n",
        "criterion = nn.CrossEntropyLoss().cuda()\n",
        "optimizer = optim.SGD(net.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
        "start = time.time()\n",
        "logfile = open('vggAtt2concatpc.csv', 'a+')\n",
        "dataheader = 'epoch, iteration, loss, train-acc, lr, time \\n'\n",
        "logfile.write(dataheader)\n",
        "logfile.close()\n",
        "for epoch in range(300):  # loop over the dataset multiple times\n",
        "    \n",
        "    if epoch % 25 == 0:\n",
        "        if epoch != 0:\n",
        "            lr = lr/2.0\n",
        "\n",
        "    optimizer = optim.SGD(net.parameters(), lr=lr, momentum=0.9, weight_decay = 5e-4)\n",
        "\n",
        "    running_loss = 0.0\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    torch.save(net.state_dict(), 'checkpointModelAtt2.pkl')\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "            # get the inputs\n",
        "            inputs, labels = data\n",
        "            #print(len(labels))\n",
        "            # wrap them in Variable\n",
        "            inputs, labels = Variable(inputs).cuda(), Variable(labels).cuda(async = True)\n",
        "            \n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            # forward + backward + optimize\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            # print statistics\n",
        "            running_loss += loss.data[0]\n",
        "            total += labels.size(0)\n",
        "            \n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            correct += predicted.eq(labels.data).cpu().sum()\n",
        "            \n",
        "            if i % 10 == 9:    # print every 10 mini-batches\n",
        "                        end = time.time()\n",
        "                        timetaken = (end - start)\n",
        "                \n",
        "                        data = ('[%d, %5d] loss: %.3f, Acc: %.3f%% (%d/%d), Learning-rate:%.8f' %\n",
        "                        (epoch + 1, i + 1, running_loss / 10,100.*correct/total, correct, total, lr))\n",
        "                        stri = ('%.3f%%' % (100.*correct/total)) \n",
        "                        print(str(data) + ' time: ' + str(timetaken))\n",
        "                        #dataline = str(epoch + 1) + ', ' + str(i+1) + ', ' + str(running_loss/10) + ', ' + str(timetaken) + ', ' + str(lr)+','+ str(stri)\n",
        "                        dataline = ('%d , %5d ,  %.3f ,  %.3f , %.8f , %.8f' %\n",
        "                        (epoch + 1, i + 1, running_loss / 10,100.*correct/total, lr, timetaken))\n",
        "                        \n",
        "                        logfile = open('vggAtt2concatpc.csv', 'a+')\n",
        "                        logfile.write(str(dataline) + '\\n')\n",
        "                        logfile.close()\n",
        "                        start = time.time()\n",
        "                        running_loss = 0.0\n",
        "                        total = 0\n",
        "                        correct = 0\n",
        "            \n",
        "    lr = lr - 1e-7\n",
        "    if lr <= 0:\n",
        "\tbreak;\n",
        "            \n",
        "print('Finished Training')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VgGioLdoRzd2",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "#testing vgg-att2-concat-pc model\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform10)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=100,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "import torch.backends.cudnn as cudnn\n",
        "net = Net().cuda()\n",
        "net = torch.nn.DataParallel(net, device_ids=range(torch.cuda.device_count()))\n",
        "cudnn.benchmark = True\n",
        "import torch.optim as optim\n",
        "import time\n",
        "\n",
        "dict_loaded = torch.load('checkpointModelAtt2.pkl')\n",
        "dict_keys_loaded = dict_loaded.keys()\n",
        "new_keys = net.state_dict().keys()\n",
        "dictionary = dict(zip(dict_keys_loaded, new_keys))\n",
        "\n",
        "new_dict = dict((dictionary[key], value) for (key, value) in dict_loaded.items())\n",
        "print(new_dict.keys())\n",
        "net.load_state_dict((new_dict ))\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "def plot_confusion_matrix(cls_pred, cls_true):\n",
        "        num_classes = 10\n",
        "        cm = confusion_matrix(y_true=cls_true,\n",
        "                              y_pred=cls_pred)\n",
        "        plt.imshow(cm, interpolation='nearest',cmap=plt.cm.jet)\n",
        "        plt.colorbar()\n",
        "        tick_marks = np.arange(num_classes)\n",
        "        plt.xticks(tick_marks, range(num_classes))\n",
        "        plt.yticks(tick_marks, range(num_classes))\n",
        "        #plt.titlr('Confusion Matrix')\n",
        "        plt.xlabel('Predicted class')\n",
        "        plt.ylabel('True class')\n",
        "        plt.show()\n",
        "        \n",
        "\n",
        "test_loss = 0\n",
        "correct = 0\n",
        "total = 0\n",
        "net.eval()\n",
        "preds = []\n",
        "trues = []\n",
        "for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "        \n",
        "        inputs, targets = Variable(inputs).cuda(), Variable(targets).cuda(async = True)\n",
        "\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        test_loss += loss.data[0]\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        preds +=(Variable(predicted).data).cpu().numpy().tolist()\n",
        "        trues += targets.data.cpu().numpy().tolist()\n",
        "        \n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets.data).cpu().sum()\n",
        "        \n",
        "        data = ( 'Loss: %.3f | Acc: %.3f%% (%d/%d)' % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
        "        print(data)\n",
        "          \n",
        "print(preds)\n",
        "print(trues)\n",
        "\n",
        "print(data)\n",
        "plt.figure()\n",
        "plot_confusion_matrix(preds, trues)\n",
        "unique, counts = np.unique(trues, return_counts=True)\n",
        "dict(zip(unique, counts))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}